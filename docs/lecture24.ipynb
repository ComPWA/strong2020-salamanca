{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 24 – Maximum Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install -q gdown==4.6.3 iminuit import-ipynb sweights\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::{dropdown} Legend\n",
    "\n",
    "> _Cursive highligted text like this indicates discussion or investigation points._\n",
    "\n",
    ":::{exercise}\n",
    "\n",
    "This indicates some exercise that could be attempted in a Jupyter or Colab notebook\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First we need to mount your drive directory and load some dependencies. Make sure your google drive system is configured\n",
    "\n",
    "\n",
    "1) go to Shared with me directory\n",
    "\n",
    "2) Right click on \"Colab - Exp Meth. Hadron Spec \" //note extra space at end\n",
    "\n",
    "2) right click and go to \"organise\" then \"add shortcut\"\n",
    "\n",
    "3) add shortcut to My Drive\n",
    "\n",
    "We will now be able to access this directory in our Colab notebook from /content/drive/MyDrive/Colab - Exp Meth. Hadron Spec /\n",
    "\n",
    "\n",
    "Now run the next cell to actually mount the drive. Note you will need to give permissions for Colab to access your google account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# mount google drive so we can access other files\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Load some helper functions from another notebook `lecture24_utilities.ipynb` by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gdown\n",
    "\n",
    "sfile = \"lecture24_utilities.ipynb\"\n",
    "if not os.path.exists(sfile):\n",
    "    url = \"https://drive.google.com/uc?id=1E899ebABgwcgfA7d-tXRud4dB7aGWIje\"\n",
    "    gdown.download(url, sfile, quiet=False)\n",
    "\n",
    "import import_ipynb  # noqa: F401\n",
    "import lecture24_utilities as s2020\n",
    "\n",
    "s2020.about()\n",
    "#!more /content/lecture24_utilities.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dealing with backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "mystnb": {
     "code_prompt_show": "Import Python libraries"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Toy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "First we will import a simple toy data set for testing techniques on. This consists of 3 variables : mass, phi, and state. These will be loaded into 3 numpy arrays.\n",
    "The data consists of signal and background events with different mass and phi distributions for each.\n",
    "\n",
    "- `mass`: Gaussian signal, log background distributions\n",
    "- `phi`: Photon Asymmetry distribution $1-h*\\Sigma*\\cos\\left(2\\phi\\right)$ with different values of $\\Sigma$ for signal and background\n",
    "- `state`: magnitude =1 for signal, 2 for background.\n",
    "polarisation state, h = sign of state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "exfile = gdown.cached_download(\n",
    "    url=\"https://drive.google.com/uc?id=15YnOFy_AE7hNBOQF4w3BRnRi2M3gYulY\",\n",
    "    path=\"data/lecture24-exampledata.txt\",\n",
    "    md5=\"e78b4dc457f52b8738e03dcdef9220d8\",\n",
    "    quiet=True,\n",
    ")\n",
    "dataArray = np.loadtxt(exfile, usecols=range(3))\n",
    "\n",
    "print(\"data file : \", dataArray)\n",
    "\n",
    "# move data into arrays given by their names\n",
    "mass = dataArray[:, 0]\n",
    "phi = dataArray[:, 1]\n",
    "state = dataArray[:, 2]\n",
    "\n",
    "# define mass range we want to use here, this can remove some background\n",
    "mrange = (0.85, 1.3)\n",
    "\n",
    "# filter events on mass range\n",
    "phi = phi[np.logical_and(mass > mrange[0], mass < mrange[1])]\n",
    "state = state[np.logical_and(mass > mrange[0], mass < mrange[1])]\n",
    "mass = mass[np.logical_and(mass > mrange[0], mass < mrange[1])]\n",
    "\n",
    "print(\"phi \", phi, phi.size)\n",
    "print(\"mass \", mass, mass.size)\n",
    "print(\"state \", state, state.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can plot each variable to see what we have. We will draw all, signal and background in different colours.\n",
    "\n",
    "Note the histSqrtErrorBars is used to plot a histogram with error bars given by sqrt of the bin contents. This will not always be correct for the plots in this notebook.\n",
    "\n",
    "First Mass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# define the binning, 100 bins in mrange\n",
    "massbins = np.linspace(mrange[0], mrange[1], 100)\n",
    "# call utility function histSqrtErrorBars to raw histograms with error bars\n",
    "mass_hist = s2020.histSqrtErrorBars(mass, massbins, \"all\")\n",
    "sig_mass_hist = s2020.histSqrtErrorBars(mass[np.abs(state) == 1], massbins, \"signal\")\n",
    "bg_mass_hist = s2020.histSqrtErrorBars(\n",
    "    mass[np.abs(state) == 2], massbins, \"background\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# define my binning for phi histograms, will use later\n",
    "phibins = np.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "phi_hist = s2020.histSqrtErrorBars(phi, phibins, \"all\")\n",
    "sig_phi_hist = s2020.histSqrtErrorBars(phi[np.abs(state) == 1], phibins, \"signal\")\n",
    "bg_phi_hist = s2020.histSqrtErrorBars(phi[np.abs(state) == 2], phibins, \"background\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The phi distributions were flat as we summed over polarisation state h. Lets select only +ve h, i.e. +ve state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "para_phi_hist = s2020.histSqrtErrorBars(phi[state > 0], phibins, \"all\")\n",
    "para_sig_phi_hist = s2020.histSqrtErrorBars(phi[state == 1], phibins, \"signal\")\n",
    "para_bg_phi_hist = s2020.histSqrtErrorBars(phi[state == 2], phibins, \"background\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "phi_fig = plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we see we have cos2ϕ distributions with signal having a larger asymmetry and background having a different signed asymmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Extended Maximum Likelihood Fit of Signal ϕ distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets try some likelihood fits to see if we can estimate $\\Sigma$ for the signal. For simplicity we will assume only a single +ve polarisation state with degree of linear polarisation = 1.\n",
    "\n",
    "(* Note normally we would see parallel or perpindicular polarisation states, but for ease of writing we use +ve and -ve labels instead.)\n",
    "\n",
    "First we must define our PDF. This is just based on a photon asymmetry distribution as seen in earlier lectures.\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma(\\phi:\\Sigma)}{d\\phi} = \\sigma_{0}(1 - \\Sigma \\cos(2ϕ))\n",
    "$$\n",
    "\n",
    "As seen the normalisation integral for this functions is $\\sigma_{0}/2\\pi$. Hence our PDF,\n",
    "\n",
    "$$\n",
    "p(\\phi:\\Sigma) = \\frac{1}{2\\pi}(1 - \\Sigma \\cos(2ϕ))\n",
    "$$\n",
    "\n",
    "Although more generally I could have the phi fit range instead of $2\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def PhotonAsymmetryPDF(xphi, Sigma):\n",
    "    \"\"\"Photon Asymmetry PDF\"\"\"\n",
    "    return (1 - Sigma * np.cos(2 * xphi)) / 2 / np.pi\n",
    "\n",
    "\n",
    "def PhotonAsymmetryN(xphi, Sigma, N):\n",
    "    \"\"\"Unormalised Photon Asymmetry\"\"\"\n",
    "    return N * PhotonAsymmetryPDF(xphi, Sigma)\n",
    "\n",
    "\n",
    "def PhotonAsymmetryNExt(xphi, Sigma, N):\n",
    "    \"\"\"Unormalised Photon Asymmetry for extended maximum likelihood\"\"\"\n",
    "    return (N, N * PhotonAsymmetryPDF(xphi, Sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And lets plot this on the signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter +ve polarisation signal events\n",
    "sig_pos_mass = mass[state == 1]\n",
    "sig_pos_phi = phi[state == 1]\n",
    "\n",
    "bg_pos_mass = mass[state == 2]\n",
    "bg_pos_phi = phi[state == 2]\n",
    "\n",
    "print(\"Number of +νₑ signal events in our data = \", sig_pos_phi.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now to estimate the height of the function for the histogram, I know I have approximaely 15,000 events and 100 bins. So average height = 150. So I might guess I can just multiply my function by 150. But if I am using the PDF I must also scale by the $2\\pi$ factor.\n",
    "\n",
    "$\\Sigma$ looks large and -ve so I will guess 0.7 for its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# plot +ve polarisation signal phi distribution\n",
    "para_sig_phi_hist = s2020.histSqrtErrorBars(sig_pos_phi, phibins, \"signal\")\n",
    "# plot a photon asymmetry function with guess parameters N=150*2pi, Sigma = -0.7\n",
    "plt.plot(\n",
    "    phibins,\n",
    "    PhotonAsymmetryN(phibins, -0.7, 150 * 2 * np.pi),\n",
    "    \"r-\",\n",
    "    label=\"first guess\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So the guess was reasonable. But lets find the best values using Maxmimum Likelihood.\n",
    "\n",
    "I need to import minimisation classes. For these exercises we will use the [`iminuit`](https://iminuit.readthedocs.io) package to perform a gradiant decent minimisation algorithm, migrad. Understanding this algorithm is very worthwhile, but not something we will cover here. Also there are other gradiant decent algothims available as well as completeley different methods such as genetic algorithms, or Bayesian methods such as MCMC or Nested Sampling which are also worth looking into for you own projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from iminuit import Minuit\n",
    "from iminuit.cost import ExtendedUnbinnedNLL, UnbinnedNLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now I need to give Minuit a function to minimise. This function is the negative log likelihood for my PDF summing over the data\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta_{j})=-\\ln(L(\\theta_{j}))  = - \\sum_{k}^N \\ln[p(x_{i,k}:\\theta_{j})] = - \\sum_{k}^N \\ln[\\frac{1}{2\\pi}(1 - \\Sigma \\cos(2ϕ))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ndata = sig_pos_phi.size\n",
    "\n",
    "# mi = Minuit( ExtendedUnbinnedNLL(phi[state==1], PhotonAsymmetryN), sigma=-0.70, N = Ndata )\n",
    "mi = Minuit(UnbinnedNLL(sig_pos_phi, PhotonAsymmetryPDF), Sigma=-0.70)\n",
    "\n",
    "# set some physical limits on parameters\n",
    "# mi.limits['N'] = (0,Ndata*1.1)\n",
    "mi.limits[\"Sigma\"] = (-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And then I can perform the migrad algorithm. Here I will also use the hesse method for better uncertainty estimation.\n",
    "\n",
    "> _Make sure you understand both algorithms, e.g. [Hesse and Minos](https://iminuit.readthedocs.io/en/stable/notebooks/hesse_and_minos.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mi.migrad()\n",
    "mi.hesse()\n",
    "\n",
    "display(mi)\n",
    "\n",
    "# save best estimate of Sigma to bestSigma\n",
    "bestSigma = mi.values[0]\n",
    "bestSigmaErr = mi.errors[0]\n",
    "# Nminuit = mi.values[1]\n",
    "\n",
    "print(\"Best value found for Sigma = \", bestSigma, \"+-\", bestSigmaErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I should have got a value close to (within 1 or 2 standard deviations) of $\\Sigma = -0.8$.\n",
    "\n",
    "Note the displayed histrogram from `iminuit` does not attempt to give a very good comparison. You need to draw a new histogram with the result superimposed. As this was not an extended maximum likelihood fit, the normalisation was not determined. We must calculate this ourselves from the number of events (dividing by nbins and scaling by 2π)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "para_sig_phi_hist = s2020.histSqrtErrorBars(sig_pos_phi, phibins, \"signal\")\n",
    "plt.plot(\n",
    "    phibins,\n",
    "    PhotonAsymmetryN(phibins, bestSigma, Ndata / phibins.size * 2 * np.pi),\n",
    "    \"r-\",\n",
    "    label=\"best fit\",\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This looks good.\n",
    "\n",
    "But the problem with data is I cannot normally seperate signal so easily. I need to use a discriminatory variable in some way. Next we will look at background subtractions via sPlots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Signal and Background fit to Mass distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this example my discriminatory variable is going to be the mass distribution. This has distinct signal (Gaussian) and background (Unknown) components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Signal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To define my signal PDF I can start from scipy norm distribution. However remember the PDF normalisation integral must be calculated within the fit ranges. Therefore I only need the integral of the distribution within these bounds rather than -∞ to ∞\n",
    "\n",
    "To get this I just use the difference of the Cumulative Density Function at the fit limits, which for the normal distribution is analytically known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def SignalMassPDF(xmass, mean, width):\n",
    "    sig = norm(mean, width)\n",
    "    # integral of signal function using CDF\n",
    "    normInt = np.diff(sig.cdf(mrange)) / (mrange[1] - mrange[0])\n",
    "    # normalised PDF\n",
    "    return sig.pdf(xmass) / normInt\n",
    "\n",
    "\n",
    "# example PDF in fit range\n",
    "plt.plot(massbins, SignalMassPDF(massbins, 1, 0.1), \"r-\", label=\"signal mass pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we are going to use Extended Maximum Likelihood for our fits as it is required for the sPlot covariance matrix.\n",
    "\n",
    "So I need to define a function for [`iminuit.cost.ExtendedUnbinnedNLL`](https://iminuit.readthedocs.io/en/stable/notebooks/interactive.html), which just multiplies the PDF by the yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def SignalMassNExt(xmass, mean, width, Ns):\n",
    "    \"\"\"Unnormalised function for extended maximum likelihood.\n",
    "\n",
    "    returns yield*PDF(mass)\n",
    "    \"\"\"\n",
    "    return (Ns, Ns * SignalMassPDF(xmass, mean, width))\n",
    "\n",
    "\n",
    "# example plot\n",
    "plt.plot(\n",
    "    massbins,\n",
    "    SignalMassNExt(massbins, 1.1, 0.05, 2_000)[1] / massbins.size,\n",
    "    \"r-\",\n",
    "    label=\"extended mass function\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And do the fit with `iminuit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "Ndata = sig_pos_mass.size\n",
    "# define iminuit extended unbinned -ve log likelihood minimisation\n",
    "mi = Minuit(\n",
    "    ExtendedUnbinnedNLL(sig_pos_mass, SignalMassNExt), mean=1.1, width=0.05, Ns=Ndata\n",
    ")\n",
    "# Set Limits\n",
    "mi.limits[\"Ns\"] = (0, Ndata * 1.1)\n",
    "mi.limits[\"mean\"] = (mrange[0], mrange[1])\n",
    "mi.limits[\"width\"] = (0.01, mrange[1] - mrange[0])\n",
    "# Fit with migrad/hesse\n",
    "mi.migrad()\n",
    "mi.hesse()\n",
    "\n",
    "display(mi)\n",
    "\n",
    "# save values\n",
    "bestMean = mi.values[0]\n",
    "bestWidth = mi.values[1]\n",
    "\n",
    "Nminuit = mi.values[2]\n",
    "\n",
    "print(\"Best value for mean = \", bestMean, \"+-\", mi.errors[0])\n",
    "print(\"Best value for width = \", bestWidth, \"+-\", mi.errors[1])\n",
    "print(\"Best value for yield = \", Nminuit, \"+-\", mi.errors[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I should get a mean around 1.1, width around 0.02 and a signal yield of around my number of data events ~15,000.\n",
    "\n",
    "The `iminuit` plot may not always looks like a great fit, it is not meant to give a correct comparison. Best to redraw the function with the best values to check the fit.\n",
    "\n",
    " Note this time the normalisation has been determined by the extended fit and I just need to account for the number of bins in my histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "para_bg_mass_hist = s2020.histSqrtErrorBars(\n",
    "    mass[state == 1], massbins, \"signal mass distribution\"\n",
    ")\n",
    "plt.plot(\n",
    "    massbins,\n",
    "    SignalMassNExt(massbins, bestMean, bestWidth, Nminuit)[1] / massbins.size,\n",
    "    \"r-\",\n",
    "    label=\"best fit\",\n",
    ")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Background Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often background distributions will not have a well defined PDF shape. As a proxy polynomial distributions can be used instead. This is likely to induce some systematic uncertainty in the final results due to mismodelling of the discriminatory variable fit, which should be investigated/estimated.\n",
    "\n",
    "When fitting polynomials as PDFs it is often the Chebyshev form of the polynomials which is used. Here we will just use predefined functions for this.\n",
    "\n",
    "> _Investigate or discuss why Chebyshev's are better to use._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "scroll-input",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# define function to return value of chebyshev polynomial in x\n",
    "# with coeffs =[c0,c1,c2,...] :\n",
    "# c0*T0(x)+c1*T1(x)+...\n",
    "\n",
    "\n",
    "def Cheb(x, coeffs):\n",
    "    return np.polynomial.chebyshev.chebval(x, coeffs)\n",
    "\n",
    "\n",
    "# for my PDF I am going to convert the range of my\n",
    "# x variable to [-1,1] to use Chebyshev\n",
    "\n",
    "\n",
    "# To calculate the normalisation integral I am going\n",
    "# to numerically sum 100 values of cheb over the\n",
    "# range [-1,1]. create sample point arrays\n",
    "# masscentres = (massbins[:-1] + massbins[1:]) / 2\n",
    "chebedges = np.arange(-1.0, 1.0, 1.0 / 1_000)\n",
    "chebcentres = (chebedges[:-1] + chebedges[1:]) / 2\n",
    "\n",
    "\n",
    "def ChebPDF(x, coeffs):\n",
    "    # transform x to 0 [-1,1]\n",
    "    x = -1 + 2 * (x - massbins[0]) / (massbins[-1] - massbins[0])\n",
    "    val = Cheb(x, coeffs)\n",
    "    # integral of function (approximate)\n",
    "    integ = np.sum(Cheb(chebcentres, coeffs)) / chebcentres.size\n",
    "    # pdf value\n",
    "    return val / integ\n",
    "\n",
    "\n",
    "# test plot\n",
    "coeffs = [0.2, -0.6, 0.3]\n",
    "plt.plot(massbins, ChebPDF(massbins, coeffs), \"r-\", label=\"signal mass pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Watch out for -ve PDF values, these are not allowed and will screw up the minimisation. In practise we should edit our function to protect against this. Here it turn out not to be an issue, (we fix c0=1) but in general IT WILL BE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now define my function for `ExtendedUnbinnedNLL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def BgMassNExt(xmass, c0, c1, c2, c3, c4, Nb):\n",
    "    # limit it to max 5 polynomial terms\n",
    "    return (Nb, Nb * ChebPDF(xmass, [c0, c1, c2, c3, c4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "Ndata = bg_pos_mass.size\n",
    "mi = Minuit(\n",
    "    ExtendedUnbinnedNLL(bg_pos_mass, BgMassNExt),\n",
    "    c0=1,\n",
    "    c1=0,\n",
    "    c2=0,\n",
    "    c3=0,\n",
    "    c4=0,\n",
    "    Nb=Ndata,\n",
    ")\n",
    "\n",
    "mi.limits[\"Nb\"] = (0, Ndata * 1.1)\n",
    "mi.limits[\"c0\"] = (-1, 1)\n",
    "mi.limits[\"c1\"] = (-1, 1)\n",
    "mi.limits[\"c2\"] = (-1, 1)\n",
    "mi.limits[\"c3\"] = (-1, 1)\n",
    "mi.limits[\"c4\"] = (-1, 1)\n",
    "\n",
    "# fix overall normalisation coefficeint to 1\n",
    "mi.fixed[\"c0\"] = True\n",
    "# Fix some coefficeints to 0 if I like\n",
    "mi.fixed[\"c1\"] = False\n",
    "mi.fixed[\"c2\"] = False\n",
    "mi.fixed[\"c3\"] = False\n",
    "mi.fixed[\"c4\"] = True\n",
    "\n",
    "mi.migrad()\n",
    "mi.hesse()\n",
    "\n",
    "display(mi)\n",
    "\n",
    "bg_c1 = mi.values[0]\n",
    "bg_c2 = mi.values[1]\n",
    "bg_c3 = mi.values[2]\n",
    "bg_c4 = mi.values[3]\n",
    "bg_c5 = mi.values[4]\n",
    "Nminuit = mi.values[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And draw the fit result with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "para_bg_mass_hist = s2020.histSqrtErrorBars(mass[state == 2], massbins, \"background\")\n",
    "plt.plot(\n",
    "    massbins,\n",
    "    BgMassNExt(massbins, bg_c1, bg_c2, bg_c3, bg_c4, bg_c5, Nminuit)[1]\n",
    "    / massbins.size,\n",
    "    \"r-\",\n",
    "    label=\"best fit\",\n",
    ")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Simulated Signal and Background PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aside : It is not always the case that a signal peak will be well fitted by a Gaussian distribution, or that a background can be easily constrained to a polynomial. In general simulations may give the best approximate PDF shapes for your event types.\n",
    "\n",
    "> _Practically how may you generate a PDF using simulated data? What do you need and what might be the problems?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Joint fit to 1D Signal and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now have our signal and background PDF functions ($p_s$ and $p_b$) which we can use in Extended Maximim Likelihood fits. To proceed we must first combine this into a single distribution for fitting i.e.\n",
    "\n",
    "$$\n",
    "f(m:Y_s,Y_b,\\mu_s,\\sigma_s,c1_b,c2_b,c3_b,c4_b) = Y_s p_s(m:\\mu_s,\\sigma_s) + Y_b p_b(m:c1_b,c2_b,c3_b,c4_b)\n",
    "$$\n",
    "\n",
    "Where $Y_{s,b}$ are the expected signal and background yields in the data.\n",
    "We then need to perform a full fit for all parameters $\\theta_j = \\{Y_s,Y_b,\\mu_s,\\sigma_s,c1_b,c2_b,c3_b,c4_b\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def CombinedMassNExt(xmass, smean, swidth, bc0, bc1, bc2, bc3, bc4, Ys, Yb):\n",
    "    return (\n",
    "        (Ys + Yb),\n",
    "        Ys * SignalMassPDF(xmass, smean, swidth)\n",
    "        + Yb * ChebPDF(xmass, [bc0, bc1, bc2, bc4, bc4]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First make our combined data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Filter all +ve polarisation events\n",
    "pos_mass = mass[state > 0]\n",
    "pos_phi = phi[state > 0]\n",
    "\n",
    "print(\"Number of +ve  events in our data = \", pos_phi.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now run `iminuit` with `ExtendedUnbinnedNLL` on our combined PDF and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "Ndata = mass.size\n",
    "mi = Minuit(\n",
    "    ExtendedUnbinnedNLL(pos_mass, CombinedMassNExt),\n",
    "    smean=bestMean,\n",
    "    swidth=bestWidth,\n",
    "    bc0=1,\n",
    "    bc1=bg_c1,\n",
    "    bc2=bg_c2,\n",
    "    bc3=bg_c3,\n",
    "    bc4=bg_c4,\n",
    "    Ys=Ndata / 2,\n",
    "    Yb=Ndata / 2,\n",
    ")\n",
    "mi.limits[\"Yb\"] = (0, Ndata * 1.1)\n",
    "mi.limits[\"Ys\"] = (0, Ndata * 1.1)\n",
    "mi.limits[\"smean\"] = (mrange[0], mrange[1])\n",
    "mi.limits[\"swidth\"] = (0.01, mrange[1] - mrange[0])\n",
    "mi.limits[\"bc0\"] = (-1, 1)\n",
    "mi.limits[\"bc1\"] = (-1, 1)\n",
    "mi.limits[\"bc2\"] = (-1, 1)\n",
    "mi.limits[\"bc3\"] = (-1, 1)\n",
    "mi.limits[\"bc4\"] = (-1, 1)\n",
    "\n",
    "# fix overall normalisation coefficeint to 1\n",
    "mi.fixed[\"bc0\"] = True\n",
    "# Fix some coefficeints to 0 if I like\n",
    "mi.fixed[\"bc1\"] = False\n",
    "mi.fixed[\"bc2\"] = False\n",
    "mi.fixed[\"bc3\"] = False\n",
    "mi.fixed[\"bc4\"] = True\n",
    "\n",
    "# do fitting\n",
    "mi.migrad()\n",
    "\n",
    "# save values\n",
    "sg_mean = mi.values[0]\n",
    "sg_width = mi.values[1]\n",
    "bg_c1 = mi.values[2]\n",
    "bg_c2 = mi.values[3]\n",
    "bg_c3 = mi.values[4]\n",
    "bg_c4 = mi.values[5]\n",
    "bg_c5 = mi.values[6]\n",
    "Ysignal = mi.values[7]\n",
    "Yback = mi.values[8]\n",
    "\n",
    "display(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And plot the fit result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "para_bg_mass_hist = s2020.histSqrtErrorBars(pos_mass, massbins, \"combined data\")\n",
    "plt.plot(\n",
    "    massbins,\n",
    "    CombinedMassNExt(\n",
    "        massbins,\n",
    "        sg_mean,\n",
    "        sg_width,\n",
    "        bg_c1,\n",
    "        bg_c2,\n",
    "        bg_c3,\n",
    "        bg_c4,\n",
    "        bg_c5,\n",
    "        Ysignal,\n",
    "        Yback,\n",
    "    )[1]\n",
    "    / massbins.size,\n",
    "    \"r-\",\n",
    "    label=\"best fit\",\n",
    ")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The fit should be pertty good although the polynomial background does not get such a good job close to threshold, if that is included in the range (Should be at 0.8). If the fit was succesfull then the results can now be used to generate weights for background subtraction (e.g. Sidebands or sPlots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## sPlots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are going to use the sweights library provided by https://sweights.readthedocs.io/en/latest/about.html\n",
    "This is related to the referenced paper on Custom Orthogonal Weight functions, of which sPlots is a specific case of. https://www.sciencedirect.com/science/article/pii/S0168900222006076?via%3Dihub\n",
    "\n",
    "One issue with sweights is it requires zero dependence between the discriminatory and physics variables used and this is often not the case. The COWS method suggests a solution for the case where there is a dependence, but it is beyond the scope of these lectures.\n",
    "\n",
    "In our toy dataset the discrimiatory (mass) and physics(phi) variables are independent for both signal and background events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sweights import SWeight  # for classic sweights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember the sWeights formula,\n",
    "\n",
    "\n",
    "$$\n",
    "w_{t,k} = \\frac{\\sum_{t'}^{N_{t}} V_{tt'}.P_{t'}(x_{k}) }{\\sum_{t'}^{N_{t}} Y_{t'}.P_{t'}(x_{k})}\n",
    "$$\n",
    "\n",
    "The `SWeight()` function will calculate the covariance matrix,\n",
    "\n",
    "$$\n",
    "\\left[ V_{tt'} \\right]^{-1} = \\frac{\\partial ^2 \\mathcal{ L}(\\theta^{best}_{j},Y_{t})}{\\partial Y_{t}Y_{t'}} = \\sum^{N_{k}}_{k}\\frac{P_{t}(x_{k})P_{t'}(x_{k})}{\\left( \\sum_{t'}^{N_{t}} Y_{t'}.P_{t'}(x_{k}) \\right)^{2}}\n",
    "$$\n",
    "\n",
    "It can use either method, taking the approximate partial derivitives from a extended maximum likelihood fit, with method=\"roofit\" $^{note}$ (left option) or by summing over the data events k with method=\"summation\" (right option).\n",
    "\n",
    "> This is indeed what RooFit does and the implementation just calls RooFit, however you need RooFit imported for that, which is not possible within Colab currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create best value PDFs for signal and background\n",
    "spdf = lambda m: SignalMassPDF(m, sg_mean, sg_width)\n",
    "bpdf = lambda m: ChebPDF(m, [bg_c2, bg_c3, bg_c4, bg_c5])\n",
    "\n",
    "# make the sweighter\n",
    "# note here I am using the fits to the positive data\n",
    "sweighter = SWeight(\n",
    "    pos_mass,\n",
    "    [spdf, bpdf],\n",
    "    [Ysignal, Yback],\n",
    "    (mrange,),\n",
    "    method=\"summation\",\n",
    "    compnames=(\"sig\", \"bkg\"),\n",
    "    verbose=True,\n",
    "    checks=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plot the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_wts(x, sw, bw, title=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, sw, \"b--\", label=\"Signal Weights\")\n",
    "    ax.plot(x, bw, \"r:\", label=\"Background Weights\")\n",
    "    ax.plot(x, sw + bw, \"k-\", label=\"Sum of Weights\")\n",
    "    ax.set_xlabel(\"Mass\")\n",
    "    ax.set_ylabel(\"Weight\")\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "# plot weights\n",
    "xaxmass = np.linspace(*mrange, 400)\n",
    "swp = sweighter.get_weight(0, xaxmass)\n",
    "bwp = sweighter.get_weight(1, xaxmass)\n",
    "plot_wts(xaxmass, swp, bwp, \"sweights\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> _Discuss if these weights look reasonable. Why are Signal Weights <0 in some regions?_\n",
    "\n",
    "Now plot the weighted phi distributions for the +ve polarisation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Get signal only weights\n",
    "sig_weights = sweighter.get_weight(0, pos_mass)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "para_phi_sig = s2020.histWeightsSqrtErrorBars(\n",
    "    pos_phi, sig_weights, phibins, \"weighted sqrt error bars\"\n",
    ")\n",
    "# draw offset sum squared weights errors\n",
    "para_phi_sig = s2020.histWeightedErrorBars(\n",
    "    pos_phi, sig_weights, phibins, \"weighted sum weightsquared error bars\", 0.05\n",
    ")\n",
    "# draw true signal distribution\n",
    "para_phi_sig_tru = s2020.histSqrtErrorBars(sig_pos_phi, phibins, \"true\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> _Investigate or discuss if these distributions and error bars look OK. Which are the correct error bars for the weighted data, blue or orange? Should weighted data have same, smaller or larger error bars than true signal events?_\n",
    "\n",
    ":::{exercise}\n",
    "\n",
    "Make phi plots for background distributions\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> _Lets consider the full mass range in the data file (draw with the next cell). We have restricted it in our test data and fits so far. Why might we a) restrict the range? b) use as large a range as we can?._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fullrange = np.linspace(0.8, 1.4, 100)\n",
    "mass_hist = s2020.histSqrtErrorBars(dataArray[:, 0], fullrange, \"fullrange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Inclusion of Weights in Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will again fit the Photon Asymmetry to extract $\\Sigma$, but now we will disentangle the signal response using the sWeights in the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here we need to define our own loss function where the log likelihood values are weighted with our background subtraction weights.\n",
    "\n",
    "The weighted negative log likelihood function,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta_{j}) = -  \\sum_{k}^{N_{k}} w_{k} \\ln[  p(x_{Pi,k}:\\theta_{Pj})] = -  \\sum_{k}^{N_{k}} w_{k} \\ln[  p(\\phi_{k}:\\Sigma)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# NLL for maximum likelihood with weights\n",
    "# Sigma = parameter to fit\n",
    "# data_phi = array of phi data\n",
    "# wk weights for signal (or background)\n",
    "\n",
    "\n",
    "def PhotonAsymWeightedNLL(Sigma, data_phi, wk):\n",
    "    pdf_vals = PhotonAsymmetryPDF(data_phi, Sigma)\n",
    "    return -np.sum(wk * np.log(pdf_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And perform the fit with `iminuit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# create negative log likelihood function I can pass to `iminuit`\n",
    "# nll = lambda s,N: PhotonAsymWeightedNLL(s,N,para_phi, sig_weights)\n",
    "nll = lambda Sigma: PhotonAsymWeightedNLL(Sigma, pos_phi, sig_weights)\n",
    "\n",
    "# create minuit fitter\n",
    "mi = Minuit(nll, Sigma=0.1)\n",
    "# mi.limits['N'] = (0,para_phi.size*1.1)\n",
    "mi.limits[\"Sigma\"] = (-1, 1)\n",
    "mi.errordef = Minuit.LIKELIHOOD\n",
    "\n",
    "mi.migrad()\n",
    "mi.hesse()\n",
    "\n",
    "\n",
    "display(mi)\n",
    "\n",
    "bestSigmaWgtd = mi.values[0]\n",
    "print(\n",
    "    \"best value for background subtracted Sigma = \",\n",
    "    bestSigmaWgtd,\n",
    "    \"+-\",\n",
    "    mi.errors[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> _How do the results and uncertainties compare to the signal only fitting? Does this seem reasonable?_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"best value for signal only data Sigma = \", bestSigma, \"+-\", bestSigmaErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can plot the fit result with the background subtracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "para_phi_sig = s2020.histWeightedErrorBars(\n",
    "    pos_phi, sig_weights, phibins, \"Signal Weighted Data\"\n",
    ")\n",
    "yld = np.sum(sig_weights)  # get signal yield from sum of the weights\n",
    "print(\"signal yield : \", yld)\n",
    "plt.plot(\n",
    "    phibins,\n",
    "    PhotonAsymmetryN(phibins, bestSigmaWgtd, yld / phibins.size * 2 * np.pi),\n",
    "    \"r-\",\n",
    "    label=\"Weighted best fit\",\n",
    ")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Uncertainties in Weighted Maximum Likelihood fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general correctly accounting for the effect of the weights on the uncertainties is a non-trivial task. For in-depth discussion of the asymptotically correct method see \"Parameter uncertainties in weighted unbinned maximum likelihood fits\", Langenbruch, https://epjc.epj.org/articles/epjc/abs/2022/05/10052_2022_Article_10254/10052_2022_Article_10254.html\n",
    "\n",
    "We will test the simplest method given therein, which is not in general correct, but can give a good approximation. It has been used in many publications. That is to add an additional factor to the sum of logs in the likelihood,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta_{j}) = - \\frac{\\sum_{k}^{N_{k}} w_{k}}{\\sum_{k}^{N_{k}} w_{k}^{2}} \\sum_{k}^{N_{k}} w_{k} \\ln[  p(x_{Pi,k}:\\theta_{Pj})]\n",
    "$$\n",
    "\n",
    "You should see a similarity between this \"sum of weights, over, sum of weights squared\" factor and our histogram uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# NLL for maximum likelihood with weights\n",
    "# See Langenbruch (19)\n",
    "\n",
    "\n",
    "def PhotonAsymWeightedW2WNLL(Sigma, data_phi, wk):\n",
    "    pdf_vals = PhotonAsymmetryPDF(data_phi, Sigma)\n",
    "    yld = np.sum(wk)\n",
    "    sw2 = np.sum(wk * wk)\n",
    "    # sum of weights/ sum of weights squared correction factor\n",
    "    return -(yld / sw2) * np.sum(wk * np.log(pdf_vals))\n",
    "    # + N - yld*np.log(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nllw2w = lambda Sigma: PhotonAsymWeightedW2WNLL(Sigma, pos_phi, sig_weights)\n",
    "mi = Minuit(nllw2w, Sigma=0.1)\n",
    "mi.errordef = Minuit.LIKELIHOOD\n",
    "\n",
    "mi.limits[\"Sigma\"] = (-1, 1)\n",
    "mi.migrad()\n",
    "mi.hesse()\n",
    "display(mi)\n",
    "\n",
    "print(\n",
    "    \"best value for background subtracted Sigma with sumw2 correction = \",\n",
    "    mi.values[0],\n",
    "    \"+-\",\n",
    "    mi.errors[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As mentioned this may be a reasonable approximation (or not) but it does not fully propogate the uncertainty. For example the uncertainty on our signal PDF parameters (mean and width) are not incorporated. Next we consider bootstrapping, which is a more time-consuming, but more robust method, for determining uncertainties as sample distributions. Note that incoporporating the sigal PDF parameter uncertainties will be left as an exercise (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## More Uncertainties : Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As outlined in the Langenbruch paper, Bootstrapping provides another independent method for estimating the uncertainties.\n",
    "\n",
    "In the paper two variations are performed\n",
    "1. Boot strap the data and use the same weights\n",
    "2. Bootstrap the data and redo the sPlot fits.\n",
    "\n",
    "Here we will follow (1) with (2), the better method, left as an exercise.\n",
    "\n",
    "Bootstrapping involves refitting the data many times. Each time using a different selection of events. Events are sampled from the original data set and crucially (to get correct variance) the sampling is done with replacements (i.e. 1 event is chosen from the same Nk events for each event in the bootstrapped data set).  The point estimates of the fit results can then be used to construct the standard deviation or confidence levels. Pretty simple, bruteforce, effective!\n",
    "\n",
    "To achieve an accurate estimate of the uncertainty a large number $O(10,000)$ of bootstrap samples should be fit.\n",
    "\n",
    "> _You can search the web to find more about bootstrapping for uncertainties, it is a common tool._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algorithm :\n",
    "\n",
    "  * Sample $N_{k}$ events (or a fraction of) from the $N_{k}$ data events $N_{boot}$ times.\n",
    "  * Each time refit the sampled data and save the result.\n",
    "\n",
    "We have a couple of control parameters,\n",
    "  \n",
    "- `Nboot`  The number of bootstrap fits to perform\n",
    "- `frac_to_sample`:  The fraction of $N_{k}$ events to use in each new bootstrap data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Bootstrap Signal Only Data\n",
    "\n",
    "First we can validate the mothod using fits to signal only data, and check we get a consistent uncertainty as we did in \"Extended Maximum Likelihood Fit of Signal ϕ distribution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# data_phi_sig_para = phi[state==1]\n",
    "Nk = sig_pos_phi.size\n",
    "\n",
    "Nboot = 1_000\n",
    "frac_to_sample = 1.0 / 1\n",
    "\n",
    "# calc, number of events per bootstrap sample\n",
    "# this could be = Nk\n",
    "Nsamp = int(Nk * frac_to_sample)\n",
    "\n",
    "# create list to save bootstrap results\n",
    "bt_sigmas = []\n",
    "\n",
    "# loop over Nboot samples\n",
    "for bt in range(Nboot):\n",
    "    # use random choice to do sampling with replacements\n",
    "    phi_bt = np.random.choice(sig_pos_phi, Nsamp)\n",
    "\n",
    "    # and do the fit for this bootstrap sample\n",
    "    btmi = Minuit(UnbinnedNLL(phi_bt, PhotonAsymmetryPDF), Sigma=-0.5)\n",
    "    btmi.limits[\"Sigma\"] = (-1, 1)\n",
    "    btmi.migrad()\n",
    "    # save results, only care about best fit value\n",
    "    bt_sigmas.append(btmi.values[0])\n",
    "bt_sigmas = np.array(bt_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(bt_sigmas, bins=100)\n",
    "print(\"the mean of bootstrap samples = \", bt_sigmas.mean())\n",
    "print(\"the standard deviation of bootstrap samples = \", bt_sigmas.std())\n",
    "print(\n",
    "    \"the scaled standard deviation of bootstrap samples = \",\n",
    "    bt_sigmas.std() * np.sqrt(frac_to_sample),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> _Why multiply the standard deviation by `sqrt(frac_to_sample)`?_\n",
    "\n",
    ":::{exercise} Complete the tables\n",
    "\n",
    "Best/mean value of $\\Sigma$ from `iminuit` and bootstrapping.\n",
    "\n",
    "| Nboot / frac  | 1  | 1/4  | 1/16\n",
    "| --------------|----|------|-----------|\n",
    "| `iminuit`     |    |  -   |-\n",
    "| 10            |    |      |\n",
    "| 100           |    |      |\n",
    "| 1,000         |    |      |\n",
    "| 5,000         |    |      |\n",
    "\n",
    "Uncertainties on $\\Sigma$ from `iminuit` and bootstrapping.\n",
    "\n",
    "| Nboot / frac  | 1  | 1/4  | 1/16\n",
    "| --------------|----|------|-----------|\n",
    "| `iminuit`     |    |  -   |-\n",
    "| 10            |    |      |\n",
    "| 100           |    |      |\n",
    "| 1,000         |    |      |\n",
    "| 5,000         |    |      |\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Bootstrap Background Subtracted Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we want to try bootstrapping on the background subtracted fits to test our uncertaintiy estimates.\n",
    "\n",
    "Algorithm :\n",
    "  * Sample $N_{k}$ events (or a fraction of) from the $N_{k}$ data events $N_{boot}$ times.\n",
    "  * Each time calculate the sPlot weights for the sampled data\n",
    "  * Then refit the sampled data using weighted nll and save the result.\n",
    "\n",
    "This is slower, so do not try too many boots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# remember to get signal weights we use sig_weights  = sweighter.get_weight(0, para_mass)\n",
    "# i.e. we are using the sweighter configured from the fit already done in \"sPlots\"\n",
    "\n",
    "# remember data is pos_phi and pos_mass\n",
    "\n",
    "Nk = pos_phi.size\n",
    "\n",
    "Nboot = 1_000\n",
    "frac_to_sample = 1.0 / 1\n",
    "Nsamp = int(Nk * frac_to_sample)\n",
    "\n",
    "bt_wgted_sigmas = []\n",
    "\n",
    "# now we need to synchronise our bootstrap samples of mass and phi\n",
    "# so we instead choose to sample the array indices\n",
    "all_indices = np.arange(Nk)\n",
    "\n",
    "for bt in range(Nboot):\n",
    "    indices_bt = np.random.choice(all_indices, Nsamp)\n",
    "    wgts_bt = sweighter.get_weight(0, pos_mass[indices_bt])\n",
    "    # now need to create nll\n",
    "    btnll = lambda Sigma: PhotonAsymWeightedNLL(Sigma, pos_phi[indices_bt], wgts_bt)\n",
    "    btmi = Minuit(btnll, Sigma=-0.1)\n",
    "    btmi.limits[\"Sigma\"] = (-1, 1)\n",
    "    btmi.migrad()\n",
    "    bt_wgted_sigmas.append(btmi.values[0])\n",
    "bt_wgted_sigmas = np.array(bt_wgted_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(bt_wgted_sigmas, bins=100)\n",
    "print(\"the mean of bootstrap samples = \", bt_wgted_sigmas.mean())\n",
    "print(\"the standard deviation of bootstrap samples = \", bt_wgted_sigmas.std())\n",
    "print(\n",
    "    \"the scaled standard deviation of bootstrap samples = \",\n",
    "    bt_wgted_sigmas.std() * np.sqrt(frac_to_sample),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{exercise}\n",
    "\n",
    "Complete the tables, using result from \"Uncertainties in Weighted Maximum Likelihood fits\n",
    "\n",
    "Best/mean value of $\\Sigma$ from weighted fits using `iminuit` and bootstrapping.\n",
    "\n",
    "| Nboot / frac  | 1  | 1/4  | 1/16\n",
    "|---------------|----|------|-----------\n",
    "| `iminuit`     |    |  -   |-\n",
    "| 10            |    |      |\n",
    "| 100           |    |      |\n",
    "| 1,000         |    |      |\n",
    "\n",
    "Uncertainties on $\\Sigma$ from weighted fits using `iminuit` and bootstrapping. Give corrected (sum of weights over sum of weights squared) and uncorrected uncertainties for `iminuit`\n",
    "\n",
    "| Nboot / frac    | 1  | 1/4  | 1/16\n",
    "|-----------------|----|------|-----------\n",
    "| `iminuit` uncor |    |  -   |-\n",
    "| `iminuit` cor   |    |  -   |-\n",
    "| 10              |    |      |\n",
    "| 100             |    |      |\n",
    "| 1,000           |    |      |\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Correcting for Acceptance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Acceptance refers to the probabilty of detecting and reconstructing your reaction at a given point in variable space $x_{i,k}={x_{0,k},x_{1,k},x_{2,k}...}$. The $x_{i,k}$ may be any measured (e.g momentum) or calculated variable (e.g. invariant mass). It can be given by the ratio of all events to those detected and reconstructed (aka accepted).\n",
    "\n",
    "$$\n",
    "η(x_{i,k}) = \\frac{f_{acc}(x_{i,k})}{f_{all}(x_{i,k})} \\approx \\frac{N^{MC}_{acc}(x_{i,l})}{N^{MC}_{gen}(x_{i,l})}\n",
    "$$\n",
    "\n",
    "This acceptance is generally approximated through Monte-Carlo simulations of detector setups. To actually calculate a value for $η(x_{i,k})$ requires some integration of MC events $x_{i,l}$ sufficiently local to $x_{i,k}$. This can be problematic if $N_{i} > 2$, i.e. if we need an acceptance in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Maximum Likelihood to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In turns out, somewhat magically, the maximum likelihood method removes the need to determine $η(x_{i,k})$ for each event. It essentially just requires a single integration over full $x_{i,k}$ space. Now, in general, the integration needs to be done for every value of model parameters (used in the fitting) instead. This however can be done more accurately, than individual acceptances, for a given number of MC events.\n",
    "\n",
    "Lets look at the equations. I have some function which describes my data that now depends on some physics model $I(x_{i}:\\theta_{j})$ and some detector acceptance $\\eta(x_{i})$,\n",
    "\n",
    "$$\n",
    "f(x_{i}:\\theta_{j}) = I(x_{i}:\\theta_{j}).\\eta(x_{i})\n",
    "$$\n",
    "\n",
    "Then I can calculate my PDF using a numerical calculation of the normalisation integral which is a summation over all $M$ accepted MC simulation events.\n",
    "\n",
    "$$\n",
    "p(x_{i}:\\theta_{j}) = \\frac{I(x_{i}:\\theta_{j})\\eta(x_{i})}{ \\sum^M_l I(x_{i,l}:\\theta_{j})  }\n",
    "$$\n",
    "\n",
    "Note, η is not required in the summation as it is explicitly included in the accepted events $M$, which is less than the number of generated MC (or thrown) events.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "L(\\theta_{j},Y) &=& \\prod_{k=0}^N p(x_{i,k}:\\theta_{j}) \\\\\n",
    "-\\ln L(\\theta_{j},Y) &=& - \\sum_{k=0}^N \\ln[\\frac{I(x_{i,k}:\\theta_{j})}{ \\sum^M_l I(x_{i,l}:\\theta_{j})}] - \\sum_k^N \\ln{\\eta(x_{i,k})}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Now we observe the last term (summing logs of acceptance) does not depend on our model parameters $\\theta_j$. Hence it is constant and can be ignored in the minimisation. The sum over $l$ MC events does depend on the parameters and so this must be recalculated for each parameter set in the minimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Toy Acceptance Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The data we have used so far has been \"perfectly\" detected. to investigate acceptance effects we need to add in these to the data. To do this we will just define some simple ϕ dependent functions, with some degraded acceptance or holes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def AcceptanceFilter(darr, condarr, acc_func):\n",
    "    \"\"\"Function to filter the data to mock up the acceptance\n",
    "\n",
    "    darr = data array to be filtered\n",
    "    condarr = data array with which to test the acceptance (does not have to be same as darr)\n",
    "    acc_func = takes a data array and returns a probability of being accepted for each entry\n",
    "    \"\"\"\n",
    "    rands = np.random.uniform(0, 1, condarr.size)\n",
    "    mask = rands[:] < acc_func(condarr)\n",
    "    return darr[mask]\n",
    "\n",
    "\n",
    "def PhiAcceptHoleFunc(phi_vals):\n",
    "    \"\"\"An acceptance function with just a hole.\"\"\"\n",
    "    return (phi_vals > 1) & (phi_vals < 2)\n",
    "\n",
    "\n",
    "def PhiAcceptSinFunc(phi_vals):\n",
    "    \"\"\"An acceptance function with a sin phi dependence\"\"\"\n",
    "    return np.sin(0.5 * phi_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# again we will just use +ve polarisation signal data\n",
    "acc_phi = AcceptanceFilter(sig_pos_phi, sig_pos_phi, PhiAcceptSinFunc)\n",
    "\n",
    "plt.hist(acc_phi, bins=100)\n",
    "plt.show()\n",
    "print(\"ndata\", acc_phi.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now lets try fitting the filtered data ignoring the acceptance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mi = Minuit(UnbinnedNLL(acc_phi, PhotonAsymmetryPDF), Sigma=-0.5)\n",
    "\n",
    "mi.migrad()\n",
    "mi.hesse()\n",
    "\n",
    "display(mi)\n",
    "\n",
    "meth0Sigma = mi.values[0]\n",
    "meth0Error = mi.errors[0]\n",
    "print(\"Best value found for Sigma = \", meth0Sigma, \"+-\", meth0Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This should probably be significantly removed from the true value of -0.8.\n",
    "Also the plots do not match at all (allthough we should not take them too seriuosly)\n",
    "\n",
    "Now lets try applying weights to correct for acceptance. This is different from the method discussed about, but since we can analyitically calculate the weights we might as well see how it does.\n",
    "\n",
    "i.e. we are going to use a likelihood like,\n",
    "\n",
    "$$\n",
    "-\\ln L(\\theta_{j},Y)  = - \\sum_{k=0}^N \\ln[\\frac{p(x_{i,k}:\\theta_{j})\\eta(x_{i,k})}{\\eta(x_{i,k})}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# create acceptance weights\n",
    "acc_wgts = PhiAcceptSinFunc(acc_phi)\n",
    "\n",
    "nll = lambda Sigma: PhotonAsymWeightedNLL(Sigma, acc_phi, acc_wgts)\n",
    "mi = Minuit(nll, Sigma=0.1)\n",
    "mi.limits[\"Sigma\"] = (-1, 1)\n",
    "\n",
    "mi.migrad()\n",
    "mi.hesse()\n",
    "\n",
    "display(mi)\n",
    "\n",
    "meth1Sigma = mi.values[0]\n",
    "meth1Error = mi.errors[0]\n",
    "print(\"Best value found for Sigma = \", meth1Sigma, \"+-\", meth1Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This should give a value much closer to -0.8.\n",
    "\n",
    "Now we can try our normalisation integral method.\n",
    "\n",
    "For this we will define a new PDF for the Photon Asymmetry where we include the acceptance in the normalisation integral.\n",
    "\n",
    "Here I am just calculating the function value at regular intervals along its range. To be more in line with using MC data I could have generated random points and used those instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ":::{exercise}\n",
    "\n",
    "Try switching to random samples rather than regular arrays for the numeric integration.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# include acceptance in normalisation integral\n",
    "integral_edges = np.arange(0, 2 * np.pi, 0.001)\n",
    "integral_centres = (integral_edges[:-1] + integral_edges[1:]) / 2\n",
    "\n",
    "\n",
    "def PhotonAsymAccPDF(xphi, Sigma):\n",
    "    # evaluate PDF over samples\n",
    "    vals = PhotonAsymmetryPDF(integral_centres, Sigma)\n",
    "    # evaluate acceptance function over samples\n",
    "    accs = PhiAcceptSinFunc(integral_centres)\n",
    "    # sum to get integral\n",
    "    integ = np.sum(vals * accs) / integral_centres.size\n",
    "    # return value of PDF\n",
    "    return PhotonAsymmetryPDF(xphi, Sigma) / integ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now use this PDF with acceptance in normalisation integral in the likelihood fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mi = Minuit(UnbinnedNLL(acc_phi, PhotonAsymAccPDF), Sigma=0.1)\n",
    "\n",
    "mi.migrad()\n",
    "mi.hesse()\n",
    "\n",
    "display(mi)\n",
    "\n",
    "meth2Sigma = mi.values[0]\n",
    "meth2Error = mi.errors[0]\n",
    "print(\"Best value found for Sigma = \", meth2Sigma, \"+-\", meth2Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hopefully the value has turned out to be close to -0.8 again! But the uncertainty is different from the previous example where we divided out the acceptance.\n",
    "\n",
    "Of course we have learnt that we may use Bootstrapping to check the uncertainties from likelihood minimisations. Lets try that here.\n",
    "\n",
    "To make it neater lets define a bootstrapper function which takes the data with acceptance and a -ve log likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def bootstrapper(all_data, Nboot, frac, nll):\n",
    "    Nsamp = int(all_data.size * frac)\n",
    "\n",
    "    bt_sigmas = []\n",
    "\n",
    "    # bootstrap loop : sample, fit, store\n",
    "    for bt in range(Nboot):\n",
    "        phi_bt = np.random.choice(all_data, Nsamp)\n",
    "        btmi = Minuit(nll(phi_bt), Sigma=0)\n",
    "        btmi.migrad()\n",
    "        bt_sigmas.append(btmi.values[0])\n",
    "    bt_sigmas = np.array(bt_sigmas)\n",
    "\n",
    "    # output results\n",
    "    plt.hist(bt_sigmas, bins=100)\n",
    "    print(\"\\n the mean of bootstrap samples = \", bt_sigmas.mean())\n",
    "    print(\"the standard deviation of bootstrap samples = \", bt_sigmas.std())\n",
    "    print(\n",
    "        \"the scaled standard deviation of bootstrap samples = \",\n",
    "        bt_sigmas.std() * np.sqrt(frac),\n",
    "    )\n",
    "    return np.array(bt_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Checking normalisation integral method\")\n",
    "print(\"Remember best value found for Sigma = \", meth2Sigma, \"+-\", meth2Error)\n",
    "# check normalisation integral method\n",
    "unll = lambda evs: UnbinnedNLL(evs, PhotonAsymAccPDF)\n",
    "sigs = bootstrapper(acc_phi, 1_000, 1.0 / 4, unll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does the bootstrap uncertainty compare to the `iminuit` one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Further Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ":::{exercise}\n",
    "\n",
    "1. Create weights for the data using a sideband method (see overview slides) and plot the sideband subtracted phi distributions. i.e. Redo the sPlots Section for sideband subtraction.\n",
    "\n",
    "2. The data contain para and perp polarized data. Currently we have used just para.Write an expanded Photon Asymmetry function to include both polarisation states and fit the signal only data.<br>\n",
    "Now try applying the acceptance function to filter the data. Try fitting without applying any acceptance correction, what do you think you will get as a result?\n",
    "\n",
    "3. For bootstrapping we do not reproduce the sWeights each fit. Should we?\n",
    "Include an sweight fit to the mass distributions of the bootstrap samples before the likelihood fit for the photon asymmetry. **Algorithm:**\n",
    "  - Sample $N_{k}$ events (or a fraction of) from the $N_{k}$ data events $N_\\mathrm{boot}$ times.\n",
    "  - Each time redo the sPlot Extended Maximum Likelihood fit and create new sweighter\n",
    "  - Then calculate the sPlot weights for the sampled data\n",
    "  - Finally, refit the sampled data using weighted nll, new weights and save the result.\n",
    "\n",
    "4. Rather than subtracting background we may perform a simultaneous fit to both signal and background distributions dependent on the discriminatory variable and control variable, i.e. mass and phi. Deduce the combined PDF function and try fitting the data with it. How do the results compare, which should be more reliable? What is the advantage of seperating the discriminatory variable fit?\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
